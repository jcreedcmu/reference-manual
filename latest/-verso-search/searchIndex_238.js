window.docContents[238].resolve({"/The--grind--tactic/Bigger-Examples/#The-Lean-Language-Reference--The--grind--tactic--Bigger-Examples--if-then-else-normalization--The-solution-using--grind":{"id":"/The--grind--tactic/Bigger-Examples/#The-Lean-Language-Reference--The--grind--tactic--Bigger-Examples--if-then-else-normalization--The-solution-using--grind","header":"17.12.2.4. The solution using  grind","context":"Lean Reference\u0009The  grind  tactic\u0009Bigger Examples\u0009if-then-else normalization","contents":"Actually solving the problem is not that hard:\nwe just need a recursive function that carries along a record of \"already assigned variables\",\nand then, whenever performing a branch on a variable, adding a new assignment in each of the branches.\nIt also needs to flatten nested if-then-else expressions which have another if-then-else in the \"condition\" position.\n(This is extracted from Chris Hughes's solution, but without the subtyping.)\n\n\n\nLet's work inside the IfExpr namespace.\n\nnamespace IfExpr\n\n\ndef normalize (assign : Std.HashMap Nat Bool) :\n    IfExpr → IfExpr\n  | lit b => lit b\n  | var v =>\n    match assign[v]? with\n    | none => var v\n    | some b => lit b\n  | ite (lit true)  t _ => normalize assign t\n  | ite (lit false) _ e => normalize assign e\n  | ite (ite a b c) t e =>\n    normalize assign (ite a (ite b t e) (ite c t e))\n  | ite (var v)     t e =>\n    match assign[v]? with\n    | none =>\n      let t' := normalize (assign.insert v true) t\n      let e' := normalize (assign.insert v false) e\n      if t' = e' then t' else ite (var v) t' e'\n    | some b => normalize assign (ite (lit b) t e)\n\n\nThis is pretty straightforward, but it immediately runs into a problem:\n\n\n\nfail to show termination for\n  IfExpr.normalize\nwith errors\nfailed to infer structural recursion:\nCannot use parameter assign:\n  the type HashMap Nat Bool does not have a `.brecOn` recursor\nCannot use parameter #2:\n  failed to eliminate recursive application\n    normalize assign (a.ite (b.ite t e) (c.ite t e))\n\n\nCould not find a decreasing measure.\nThe basic measures relate at each recursive call as follows:\n(<, ≤, =: relation proved, ? all proofs failed, _: no proof attempted)\n              #1 x2\n1) 1296:27-45  =  <\n2) 1297:27-45  =  <\n3) 1299:4-52   =  ?\n4) 1303:16-50  ?  _\n5) 1304:16-51  _  _\n6) 1306:16-50  _  _\n\n#1: assign\n\nPlease use `termination_by` to specify a decreasing measure.\n\n\nLean here is telling us that it can't see that the function is terminating.\nOften Lean is pretty good at working this out for itself, but for sufficiently complicated functions\nwe need to step in to give it a hint.\n\nIn this case we can see that it's the recursive call\nite (ite a b c) t e which is calling normalize on (ite a (ite b t e) (ite c t e))\nwhere Lean is having difficulty. Lean has made a guess at a plausible termination measure,\nbased on using automatically generated sizeOf function, but can't prove the resulting goal,\nessentially because t and e appear multiple times in the recursive call.\n\nTo address problems like this, we nearly always want to stop using the automatically generated sizeOf function,\nand construct our own termination measure. We'll use\n\n@[simp] def normSize : IfExpr → Nat\n  | lit _ => 0\n  | var _ => 1\n  | .ite i t e => 2 * normSize i + max (normSize t) (normSize e) + 1\n\n\nMany different functions would work here. The basic idea is to increase the \"weight\" of the \"condition\" branch\n(this is the multiplicative factor in the 2 * normSize i ),\nso that as long the \"condition\" part shrinks a bit, the whole expression counts as shrinking even if the \"then\" and \"else\" branches have grown.\nWe've annotated the definition with @[simp] so Lean's automated termination checker is allowed to unfold the definition.\n\nWith this in place, the definition goes through using the termination_by clause:\n\ndef normalize (assign : Std.HashMap Nat Bool) :\n    IfExpr → IfExpr\n  | lit b => lit b\n  | var v =>\n    match assign[v]? with\n    | none => var v\n    | some b => lit b\n  | ite (lit true)  t _ => normalize assign t\n  | ite (lit false) _ e => normalize assign e\n  | ite (ite a b c) t e =>\n    normalize assign (ite a (ite b t e) (ite c t e))\n  | ite (var v)     t e =>\n    match assign[v]? with\n    | none =>\n      let t' := normalize (assign.insert v true) t\n      let e' := normalize (assign.insert v false) e\n      if t' = e' then t' else ite (var v) t' e'\n    | some b => normalize assign (ite (lit b) t e)\ntermination_by e => e.normSize\nNow it's time to prove some properties of this function.\nWe're just going to package together all the properties we want:theorem normalize_spec\n    (assign : Std.HashMap Nat Bool) (e : IfExpr) :\n    (normalize assign e).normalized\n      ∧ (∀ f, (normalize assign e).eval f =\n          e.eval fun w => assign[w]?.getD (f w))\n      ∧ ∀ (v : Nat),\n          v ∈ vars (normalize assign e) → ¬ v ∈ assign :=\n  sorry\nThat is:* the result of normalize is actually normalized according to the initial definitions,* if we normalize an \"if-then-else\" expression using some assignments, and then evaluate the remaining variables,\n  we get the same result as evaluating the original \"if-then-else\" using the composite of the two assignments,* and any variable appearing in the assignments no longer appears in the normalized expression.You might think that we should state these three properties as separate lemmas,\nbut it turns out that proving them all at once is really convenient, because we can use the fun_induction\ntactic to assume that all these properties hold for normalize in the recursive calls, and then\ngrind will just put all the facts together for the result:-- We tell `grind` to unfold our definitions above.\nattribute [local grind]\n  normalized hasNestedIf hasConstantIf hasRedundantIf\n  disjoint vars eval List.disjoint\n\ntheorem normalize_spec\n    (assign : Std.HashMap Nat Bool) (e : IfExpr) :\n    (normalize assign e).normalized\n      ∧ (∀ f, (normalize assign e).eval f =\n          e.eval fun w => assign[w]?.getD (f w))\n      ∧ ∀ (v : Nat),\n          v ∈ vars (normalize assign e) → ¬ v ∈ assign := by\n  fun_induction normalize with grind\nThe fact that the fun_induction plus grind combination just works here is sort of astonishing.\nWe're really excited about this, and we're hoping to see a lot more proofs in this style!A lovely consequence of highly automated proofs is that often you have some flexibility to change the statements,\nwithout changing the proof at all! As examples, the particular way that we asserted above that\n\"any variable appearing in the assignments no longer appears in the normalized expression\"\ncould be stated in many different ways (although not omitted!). The variations really don't matter,\nand grind can both prove, and use, any of them:Here we use assign.contains v = false:example (assign : Std.HashMap Nat Bool) (e : IfExpr) :\n    (normalize assign e).normalized\n      ∧ (∀ f, (normalize assign e).eval f =\n          e.eval fun w => assign[w]?.getD (f w))\n      ∧ ∀ (v : Nat), v ∈ vars (normalize assign e) →\n          assign.contains v = false := by\n  fun_induction normalize with grind\nand here we use assign[v]? = none:example (assign : Std.HashMap Nat Bool) (e : IfExpr) :\n    (normalize assign e).normalized\n      ∧ (∀ f, (normalize assign e).eval f =\n          e.eval fun w => assign[w]?.getD (f w))\n      ∧ ∀ (v : Nat),\n          v ∈ vars (normalize assign e) → assign[v]? = none := by\n  fun_induction normalize with grind\nIn fact, it's also of no consequence to grind whether we use a\nHashMap or a TreeMap to store the assignments,\nwe can simply switch that implementation detail out, without having to touch the proofs:\n\n\n\ndef normalize (assign : Std.TreeMap Nat Bool) :\n    IfExpr → IfExpr\n  | lit b => lit b\n  | var v =>\n    match assign[v]? with\n    | none => var v\n    | some b => lit b\n  | ite (lit true)  t _ => normalize assign t\n  | ite (lit false) _ e => normalize assign e\n  | ite (ite a b c) t e =>\n    normalize assign (ite a (ite b t e) (ite c t e))\n  | ite (var v)     t e =>\n    match assign[v]? with\n    | none =>\n      let t' := normalize (assign.insert v true) t\n      let e' := normalize (assign.insert v false) e\n      if t' = e' then t' else ite (var v) t' e'\n    | some b => normalize assign (ite (lit b) t e)\ntermination_by e => e.normSize\n\ntheorem normalize_spec\n    (assign : Std.TreeMap Nat Bool) (e : IfExpr) :\n    (normalize assign e).normalized\n      ∧ (∀ f, (normalize assign e).eval f =\n          e.eval fun w => assign[w]?.getD (f w))\n      ∧ ∀ (v : Nat),\n          v ∈ vars (normalize assign e) → ¬ v ∈ assign := by\n  fun_induction normalize with grind\n\n\n(The fact that we can do this relies on the fact that all the lemmas for both HashMap and for TreeMap that grind needs have already be annotated in the standard library.)\n\nIf you'd like to play around with this code,\nyou can find the whole file here,\nor in fact play with it with no installation\nin the Live Lean editor.\n\n\n\n"},"/Notations-and-Macros/Macros/#macro-hygiene":{"id":"/Notations-and-Macros/Macros/#macro-hygiene","header":"20.5.1. Hygiene","context":"Lean Reference\u0009Notations and Macros\u0009Macros","contents":"A macro is hygienic if its expansion cannot result in identifier capture.\nIdentifier capture is when an identifier ends up referring to a binding site other than that which is in scope where the identifier occurs in the source code.\nThere are two types of identifier capture:\n\n* If a macro's expansion introduces binders, then identifiers that are parameters to the macro may end up referring to the introduced binders if their names happen to match.* If a macro's expansion is intended to refer to a name, but the macro is used in a context that either locally binds this name or in which a new global name has been introduced, it may end up referring to the wrong name.\n\nThe first kind of variable capture can be avoided by ensuring that every binding introduced by a macro uses a freshly generated, globally-unique name, while the second can be avoided by always using fully-qualified names to refer to constants.\nThe fresh names must be generated again at each invocation of the macro to avoid variable capture in recursive macros.\nThese techniques are error-prone.\nVariable capture issues are difficult to test for because they rely on coincidences of name choices, and consistently applying these techniques results in noisy code.\n\nLean features automatic hygiene: in almost all cases, macros are automatically hygienic.\nCapture by introduced bindings is avoided by annotating identifiers introduced by a macro with macro scopes, which uniquely identify each invocation of macro expansion.\nIf the binding and the use of the identifier have the same macro scopes, then they were introduced by the same step of macro expansion and should refer to one another.\nSimilarly, uses of global names in code generated by a macro are not captured by local bindings in the context in which they are expanded because these use sites have macro scopes that are not present in the binding occurrence.\nCapture by newly-introduced global names is prevented by annotating potential global name references with the set of global names that match at quotation time in code produced in the macro's body.\nIdentifiers annotated with potential referents are called pre-resolved identifiers, and the Syntax.Preresolved field on the Syntax.ident constructor is used to store the potential referents.\nDuring elaboration, if an identifier has pre-resolved global names associated with it, then other global names are not considered as valid reference targets.\n\nThe introduction of macro scopes and pre-resolved identifiers to generated syntax occurs during quotation.\nMacros that construct syntax by other means than quotation should also ensure hygiene by some other means.\nFor more details on Lean's hygiene algorithm, please consult .\n\n"},"/Functors___-Monads-and--do--Notation/Varieties-of-Monads/#The-Lean-Language-Reference--Functors___-Monads-and--do--Notation--Varieties-of-Monads--Combined-Error-and-State-Monads":{"id":"/Functors___-Monads-and--do--Notation/Varieties-of-Monads/#The-Lean-Language-Reference--Functors___-Monads-and--do--Notation--Varieties-of-Monads--Combined-Error-and-State-Monads","header":"14.5.8. Combined Error and State Monads","context":"Lean Reference\u0009Functors, Monads and  do -Notation\u0009Varieties of Monads","contents":"\n\nThe EStateM monad has both exceptions and mutable state.\nEStateM ε σ α is logically equivalent to ExceptT ε (StateM σ) α.\nWhile ExceptT ε (StateM σ) evaluates to the type σ → Except ε α × σ, the type EStateM ε σ α evaluates to σ → EStateM.Result ε σ α.\nEStateM.Result is an inductive type that's very similar to Except, except both constructors have an additional field for the state.\nIn compiled code, this representation removes one level of indirection from each monadic bind.\n\n\n\nA combined state and exception monad in which exceptions do not automatically roll back the state.Instances of EStateM.Backtrackable provide a way to roll back some part of the state if needed.EStateM ε σ is equivalent to ExceptT ε (StateM σ), but it is more efficient.\n\nThe value returned from a combined state and exception monad in which exceptions do not\nautomatically roll back the state.Result ε σ α is equivalent to Except ε α × σ, but using a single combined inductive type yields\na more efficient data representation.A success value of type α and a new state σ.An exception of type ε and a new state σ.\n\nExecutes an EStateM action with the initial state s. The returned value includes the final state\nand indicates whether an exception was thrown or a value was returned.\n\nExecutes an EStateM with the initial state s for the returned value α, discarding the final\nstate. Returns none if an unhandled exception was thrown.\n\nTransforms exceptions with a function, doing nothing on successful results.\n\nConverts a state monad action into a state monad action with exceptions.The resulting action does not throw an exception.\n\n\n\n\n\n"},"/Basic-Types/Tuples/#tuples":{"id":"/Basic-Types/Tuples/#tuples","header":"19.13. Tuples","context":"Lean Reference\u0009Basic Types","contents":"The Lean standard library includes a variety of tuple-like types.\nIn practice, they differ in four ways:* whether the first projection is a type or a proposition* whether the second projection is a type or a proposition* whether the second projection's type depends on the first projection's value* whether the type as a whole is a proposition or type\n\n* Type* First Projection* Second Projection* Dependent?* Universe* Prod* Type u* Type v* ❌️* Type (max u v)* And* Prop* Prop* ❌️* Prop* Sigma* Type u* Type v* ✔* Type (max u v)* Subtype* Type u* Prop* ✔* Type u* Exists* Type u* Prop* ✔* Prop\n\nSome potential rows in this table do not exist in the library:* There is no dependent pair where the first projection is a proposition, because proof irrelevance renders this meaningless.* There is no non-dependent pair that combines a type with a proposition because the situation is rare in practice: grouping data with unrelated proofs is uncommon.\n\nThese differences lead to very different use cases.\nProd and its variants PProd and MProd simply group data together—they are products.\nBecause its second projection is dependent, Sigma has the character of a sum: for each element of the first projection's type, there may be a different type in the second projection.\nSubtype selects the values of a type that satisfy a predicate.\nEven though it syntactically resembles a pair, in practice it is treated as an actual subset.\nAnd is a logical connective, and Exists is a quantifier.\nThis chapter documents the tuple-like pairs, namely Prod and Sigma.\n\n\n\n\n\n"}});